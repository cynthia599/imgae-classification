# train_cpu.py - å®Œæ•´çš„è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒGPUï¼‰
import torch
import torch.nn as nn
from data_loader import get_stl10_dataloaders
from model import ImprovedCNN
from utils import evaluate_model, setup_optimizer, plot_optimizer_comparison, plot_loss_comparison, plot_training_curves_comparison, plot_class_accuracy, generate_optimizer_table, print_experiment_summary, STL10_class_names
import os
import pickle
import traceback
import numpy as np
import matplotlib.pyplot as plt

print("å½“å‰å·¥ä½œç›®å½•:", os.getcwd())
print("æ–‡ä»¶ä¿å­˜è·¯å¾„:", os.path.abspath('.'))
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

def train_single_experiment(optimizer_name='adam', learning_rate=0.001, batch_size=32, epochs=50, momentum=0.9):
    """è®­ç»ƒå•ä¸ªä¼˜åŒ–å™¨å®éªŒ - ä½¿ç”¨GPU"""
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type == 'cuda':
        print(f"ğŸš€ GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # è·å–æ•°æ®
    train_loader, val_loader, test_loader = get_stl10_dataloaders(batch_size)
    
    model = ImprovedCNN().to(device)
    
    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = setup_optimizer(model, optimizer_name, learning_rate, momentum)
    
    print(f"\nå¼€å§‹è®­ç»ƒ {optimizer_name} ä¼˜åŒ–å™¨...")
    print(f"è®¾å¤‡: {device}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"æ•°æ®é›†: STL-10")
    print(f"æ¨¡å‹: ImprovedCNN (å¸¦æ‰¹é‡å½’ä¸€åŒ–)")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    if optimizer_name == 'sgd_momentum':
        print(f"åŠ¨é‡å‚æ•°: {momentum}")
    
    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    train_losses = []
    val_losses = []
    val_accuracies = []
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        # ========== è®­ç»ƒé˜¶æ®µ ==========
        model.train()
        running_loss = 0.0
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            
            # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if batch_idx % 50 == 0:
                print(f'Epoch: {epoch+1}/{epochs} | Batch: {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}')
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # ========== éªŒè¯é˜¶æ®µ ==========
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        # è®¡ç®—éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        accuracy = 100 * correct / total
        val_accuracies.append(accuracy)
        
        # æ¯5ä¸ªepochæ‰“å°è¯¦ç»†è¿›åº¦
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f'Epoch [{epoch+1}/{epochs}] | è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | éªŒè¯æŸå¤±: {avg_val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}%')
            
            # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if device.type == 'cuda':
                memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                memory_cached = torch.cuda.memory_reserved(0) / 1024**3
                print(f"ğŸ’¾ GPUå†…å­˜ - å·²åˆ†é…: {memory_allocated:.2f}GB, ç¼“å­˜: {memory_cached:.2f}GB")
    
    # æœ€ç»ˆè¯„ä¼°ä½¿ç”¨æµ‹è¯•é›†
    final_accuracy, class_accuracy = evaluate_model(model, test_loader, device)
    
    # æ¸…ç†GPUç¼“å­˜
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    
    return {
        'model': model,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies,
        'final_accuracy': final_accuracy,
        'class_accuracy': class_accuracy,
        'model_type': 'ImprovedCNN',
        'optimizer_name': optimizer_name
    }

def generate_experiment_summary_chart(results, save_path='experiment_summary.png'):
    """ç”Ÿæˆå®éªŒæ±‡æ€»å›¾è¡¨"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    optimizers = list(results.keys())
    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']
    
    # 1. æ‰€æœ‰ä¼˜åŒ–å™¨çš„éªŒè¯å‡†ç¡®ç‡
    for idx, (opt_name, result) in enumerate(results.items()):
        axes[0, 0].plot(result['val_accuracies'], 
                       color=colors[idx % len(colors)],
                       linewidth=2,
                       label=opt_name.upper())
    
    axes[0, 0].set_title('æ‰€æœ‰ä¼˜åŒ–å™¨éªŒè¯å‡†ç¡®ç‡å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('è®­ç»ƒè½®æ•°')
    axes[0, 0].set_ylabel('å‡†ç¡®ç‡ (%)')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_ylim([0, 100])
    
    # 2. æ‰€æœ‰ä¼˜åŒ–å™¨çš„è®­ç»ƒæŸå¤±
    for idx, (opt_name, result) in enumerate(results.items()):
        axes[0, 1].plot(result['train_losses'], 
                       color=colors[idx % len(colors)],
                       linewidth=2,
                       label=opt_name.upper())
    
    axes[0, 1].set_title('æ‰€æœ‰ä¼˜åŒ–å™¨è®­ç»ƒæŸå¤±å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('è®­ç»ƒè½®æ•°')
    axes[0, 1].set_ylabel('æŸå¤±å€¼')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æœ€ç»ˆå‡†ç¡®ç‡æŸ±çŠ¶å›¾
    opt_names = []
    final_accs = []
    for opt_name, result in results.items():
        opt_names.append(opt_name.upper())
        final_accs.append(result['final_accuracy'])
    
    bars = axes[1, 0].bar(opt_names, final_accs, 
                          color=colors[:len(opt_names)], 
                          edgecolor='black')
    axes[1, 0].set_title('æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”', fontsize=12, fontweight='bold')
    axes[1, 0].set_ylabel('å‡†ç¡®ç‡ (%)')
    axes[1, 0].grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, acc in zip(bars, final_accs):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')
    
    # 4. æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ
    convergence_data = []
    for opt_name, result in results.items():
        if 'val_accuracies' in result:
            max_acc = max(result['val_accuracies'])
            target_90 = max_acc * 0.9
            
            convergence_epoch = 0
            for epoch, acc in enumerate(result['val_accuracies']):
                if acc >= target_90:
                    convergence_epoch = epoch + 1
                    break
            if convergence_epoch == 0:
                convergence_epoch = len(result['val_accuracies'])
            
            convergence_data.append((opt_name.upper(), convergence_epoch))
    
    conv_names = [x[0] for x in convergence_data]
    conv_epochs = [x[1] for x in convergence_data]
    
    bars2 = axes[1, 1].bar(conv_names, conv_epochs, 
                           color=colors[:len(conv_names)], 
                           edgecolor='black')
    axes[1, 1].set_title('æ”¶æ•›é€Ÿåº¦å¯¹æ¯” (è¾¾åˆ°90%æœ€å¤§å‡†ç¡®ç‡)', fontsize=12, fontweight='bold')
    axes[1, 1].set_ylabel('æ‰€éœ€è½®æ•°')
    axes[1, 1].grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, epochs in zip(bars2, conv_epochs):
        height = bar.get_height()
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{epochs}', ha='center', va='bottom', fontweight='bold')
    
    plt.suptitle('STL-10 ä¼˜åŒ–å™¨å®éªŒç»“æœæ±‡æ€»', fontsize=14, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"âœ… å®éªŒæ±‡æ€»å›¾å·²ä¿å­˜: {save_path}")
    return fig

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œä¸åŒä¼˜åŒ–å™¨çš„å®éªŒ"""
    
    # å®šä¹‰è¦æµ‹è¯•çš„ä¼˜åŒ–å™¨
    optimizers = ['adam', 'sgd', 'sgd_momentum', 'adagrad', 'rmsprop']
    
    # ä¸ºä¸åŒä¼˜åŒ–å™¨è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    learning_rates = {
        'adam': 0.001,
        'sgd': 0.01,
        'sgd_momentum': 0.01,
        'adagrad': 0.01,
        'rmsprop': 0.001
    }
    
    results = {}
    
    for opt_name in optimizers:
        print(f"\n{'='*50}")
        print(f"è®­ç»ƒ {opt_name.upper()} ä¼˜åŒ–å™¨")
        print(f"{'='*50}")
        
        # è®­ç»ƒå•ä¸ªä¼˜åŒ–å™¨
        result = train_single_experiment(
            optimizer_name=opt_name,
            learning_rate=learning_rates[opt_name],
            batch_size=32,
            epochs=50,
            momentum=0.9
        )
        
        results[opt_name] = result
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\n{opt_name.upper()} ä¼˜åŒ–å™¨æœ€ç»ˆå‡†ç¡®ç‡: {result['final_accuracy']:.2f}%")
    
    # ========== ä¿å­˜ç»“æœéƒ¨åˆ† ==========
    print("\n" + "="*60)
    print("å¼€å§‹ä¿å­˜è®­ç»ƒç»“æœå’Œå›¾è¡¨...")
    print("="*60)
    
    try:
        # 1. åˆ›å»ºå›¾è¡¨ä¿å­˜ç›®å½•
        charts_dir = "training_charts"
        os.makedirs(charts_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºå›¾è¡¨ç›®å½•: {charts_dir}")
        
        # 2. ç»˜åˆ¶ä¼˜åŒ–å™¨æ¯”è¾ƒå›¾
        print("\nğŸ“Š ç”Ÿæˆä¼˜åŒ–å™¨æ¯”è¾ƒå›¾...")
        plot_optimizer_comparison(results)
        
        # 3. ç»˜åˆ¶æŸå¤±å‡½æ•°å¯¹æ¯”å›¾
        print("ğŸ“ˆ ç”ŸæˆæŸå¤±å‡½æ•°å¯¹æ¯”å›¾...")
        plot_loss_comparison(results)
        
        # 4. ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾
        print("ğŸ“ˆ ç”Ÿæˆè®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾...")
        plot_training_curves_comparison(results, 'training_curves.png')
        
        # 5. ä¸ºæ¯ä¸ªä¼˜åŒ–å™¨ç»˜åˆ¶è¯¦ç»†å›¾è¡¨
        print("\nğŸ“Š ä¸ºæ¯ä¸ªä¼˜åŒ–å™¨ç”Ÿæˆè¯¦ç»†å›¾è¡¨...")
        for opt_name, result in results.items():
            print(f"  æ­£åœ¨å¤„ç† {opt_name.upper()}...")
            
            # 5.1 è®­ç»ƒå†å²æ›²çº¿
            fig, axes = plt.subplots(1, 2, figsize=(12, 4))
            
            epochs = range(1, len(result['train_losses']) + 1)
            
            # è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±
            axes[0].plot(epochs, result['train_losses'], 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±')
            axes[0].plot(epochs, result['val_losses'], 'r-', linewidth=2, label='éªŒè¯æŸå¤±')
            axes[0].set_title(f'{opt_name.upper()} - æŸå¤±æ›²çº¿', fontsize=12, fontweight='bold')
            axes[0].set_xlabel('è®­ç»ƒè½®æ•°')
            axes[0].set_ylabel('æŸå¤±å€¼')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # éªŒè¯å‡†ç¡®ç‡
            axes[1].plot(epochs, result['val_accuracies'], 'g-', linewidth=2)
            axes[1].set_title(f'{opt_name.upper()} - éªŒè¯å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
            axes[1].set_xlabel('è®­ç»ƒè½®æ•°')
            axes[1].set_ylabel('å‡†ç¡®ç‡ (%)')
            axes[1].grid(True, alpha=0.3)
            axes[1].set_ylim([0, 100])
            
            plt.suptitle(f'{opt_name.upper()} ä¼˜åŒ–å™¨ - è®­ç»ƒè¯¦æƒ…', fontsize=14, fontweight='bold', y=1.02)
            plt.tight_layout()
            plt.savefig(f'{charts_dir}/training_history_{opt_name}.png', 
                       dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            print(f"    âœ… {opt_name}è®­ç»ƒå†å²å›¾å·²ä¿å­˜: {charts_dir}/training_history_{opt_name}.png")
            
            # 5.2 å•ç‹¬çš„ç±»åˆ«å‡†ç¡®ç‡å›¾
            if 'class_accuracy' in result and result['class_accuracy'] is not None:
                plot_class_accuracy(result['class_accuracy'], opt_name, 
                                   f'{charts_dir}/class_accuracy_{opt_name}.png')
                print(f"    âœ… {opt_name}ç±»åˆ«å‡†ç¡®ç‡å›¾å·²ä¿å­˜: {charts_dir}/class_accuracy_{opt_name}.png")
        
        # 6. ç”Ÿæˆæœ€ä½³ä¼˜åŒ–å™¨çš„ç±»åˆ«å‡†ç¡®ç‡å›¾
        print("\nğŸ† ç”Ÿæˆæœ€ä½³ä¼˜åŒ–å™¨çš„è¯¦ç»†å›¾è¡¨...")
        best_opt = max(results.keys(), key=lambda x: results[x]['final_accuracy'])
        best_result = results[best_opt]
        
        # 6.1 æœ€ä½³ä¼˜åŒ–å™¨çš„ç±»åˆ«å‡†ç¡®ç‡å›¾
        if 'class_accuracy' in best_result and best_result['class_accuracy'] is not None:
            plot_class_accuracy(best_result['class_accuracy'], f"{best_opt}(æœ€ä½³)", 
                               'class_accuracy.png')
            print(f"âœ… æœ€ä½³ä¼˜åŒ–å™¨ç±»åˆ«å‡†ç¡®ç‡å›¾å·²ä¿å­˜: class_accuracy.png")
        
        # 6.2 åˆ›å»ºå®éªŒæ±‡æ€»å›¾è¡¨
        print("\nğŸ“‹ åˆ›å»ºå®éªŒæ±‡æ€»å›¾è¡¨...")
        generate_experiment_summary_chart(results, 'experiment_summary.png')
        
        # 7. ç”Ÿæˆä¼˜åŒ–å™¨å¯¹æ¯”è¡¨æ ¼
        print("\nğŸ“‹ ç”Ÿæˆä¼˜åŒ–å™¨å¯¹æ¯”è¡¨æ ¼...")
        generate_optimizer_table(results)
        
        # 8. æ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨çš„æ€§èƒ½
        print_experiment_summary(results)
        print("âœ… å®éªŒæ€»ç»“å·²ç”Ÿæˆ")
        
        # 9. ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        print("\nğŸ’¾ ä¿å­˜è®­ç»ƒæ•°æ®...")
        with open('training_results.pkl', 'wb') as f:
            pickle.dump(results, f)
        print("âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: training_results.pkl")
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰è®­ç»ƒç»“æœå’Œå›¾è¡¨ä¿å­˜å®Œæˆï¼")
        print("="*60)
        print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨:")
        print("â”œâ”€â”€ accuracy_comparison.png (ä¼˜åŒ–å™¨å‡†ç¡®ç‡å¯¹æ¯”)")
        print("â”œâ”€â”€ loss_comparison.png (æŸå¤±å‡½æ•°å¯¹æ¯”)")
        print("â”œâ”€â”€ training_curves.png (è®­ç»ƒæ›²çº¿å¯¹æ¯”)")
        print("â”œâ”€â”€ class_accuracy.png (æœ€ä½³ä¼˜åŒ–å™¨ç±»åˆ«å‡†ç¡®ç‡)")
        print("â”œâ”€â”€ experiment_summary.png (å®éªŒæ±‡æ€»å›¾)")
        print("â”œâ”€â”€ optimizer_comparison_table.png (ä¼˜åŒ–å™¨å¯¹æ¯”è¡¨)")
        print("â”œâ”€â”€ optimizer_comparison_data.csv (ä¼˜åŒ–å™¨å¯¹æ¯”æ•°æ®)")
        print("â”œâ”€â”€ training_results.pkl (è®­ç»ƒæ•°æ®)")
        print("â””â”€â”€ training_charts/ (å„ä¼˜åŒ–å™¨è¯¦ç»†å›¾è¡¨ç›®å½•)")
        print("    â”œâ”€â”€ training_history_[optimizer].png (å„ä¼˜åŒ–å™¨è®­ç»ƒå†å²)")
        print("    â””â”€â”€ class_accuracy_[optimizer].png (å„ä¼˜åŒ–å™¨ç±»åˆ«å‡†ç¡®ç‡)")
        
        # åˆ—å‡ºå®é™…ç”Ÿæˆçš„æ–‡ä»¶
        print(f"\nğŸ“ å½“å‰ç›®å½•ç”Ÿæˆçš„æ–‡ä»¶:")
        file_count = 0
        for f in sorted(os.listdir('.')):
            if f.endswith('.png') or f.endswith('.pkl') or f.endswith('.csv'):
                file_size = os.path.getsize(f) / 1024  # KB
                print(f"   - {f:35} ({file_size:.1f} KB)")
                file_count += 1
        
        print(f"\nğŸ“ {charts_dir} ç›®å½•ä¸­çš„æ–‡ä»¶:")
        if os.path.exists(charts_dir):
            chart_count = 0
            for f in sorted(os.listdir(charts_dir)):
                if f.endswith('.png'):
                    file_size = os.path.getsize(os.path.join(charts_dir, f)) / 1024
                    print(f"   - {charts_dir}/{f:30} ({file_size:.1f} KB)")
                    chart_count += 1
            print(f"  æ€»å…± {chart_count} ä¸ªå›¾è¡¨æ–‡ä»¶")
        
        print(f"\nâœ… æ€»å…±ç”Ÿæˆ {file_count} ä¸ªä¸»æ–‡ä»¶å’Œ {chart_count if 'chart_count' in locals() else 0} ä¸ªå›¾è¡¨æ–‡ä»¶")
        
    except Exception as e:
        print(f"\nâŒ ä¿å­˜å¤±è´¥: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
    
    return results

if __name__ == "__main__":
    # æ£€æŸ¥GPUçŠ¶æ€
    if torch.cuda.is_available():
        print("ğŸ‰ ä½¿ç”¨GPUè¿›è¡Œè®­ç»ƒ!")
        print(f"ğŸš€ GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸ ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ (GPUä¸å¯ç”¨)")
    
    print("æµ‹è¯•çš„ä¼˜åŒ–å™¨åŒ…æ‹¬: Adam, SGD, SGD with Momentum, Adagrad, RMSprop")
    print("ä½¿ç”¨çš„æ¨¡å‹: ImprovedCNN (å¸¦æ‰¹é‡å½’ä¸€åŒ–)")
    print("è®­ç»ƒè½®æ•°: 50 epochs")
    print("å¼€å§‹è®­ç»ƒ...")
    
    results = main()
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset, random_split, Subset
import platform
import os
import numpy as np

# è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œç”¨äº Mean Teacher çš„æ— æ ‡ç­¾æ•°æ®
class MeanTeacherUnlabeledDataset(Dataset):
    """
    Mean Teacher ä¸“ç”¨çš„æ— æ ‡ç­¾æ•°æ®é›†
    ä¸ºæ¯ä¸ªæ ·æœ¬æä¾›ä¸¤ç§ä¸åŒå¢å¼ºï¼šå¼±å¢å¼ºï¼ˆæ•™å¸ˆï¼‰å’Œå¼ºå¢å¼ºï¼ˆå­¦ç”Ÿï¼‰
    """
    
    def __init__(self, stl10_dataset, weak_transform, strong_transform):
        """
        å‚æ•°:
            stl10_dataset: STL-10 æ— æ ‡ç­¾æ•°æ®é›†
            weak_transform: å¼±å¢å¼ºå˜æ¢ï¼ˆç”¨äºæ•™å¸ˆæ¨¡å‹ï¼‰
            strong_transform: å¼ºå¢å¼ºå˜æ¢ï¼ˆç”¨äºå­¦ç”Ÿæ¨¡å‹ï¼‰
        """
        self.dataset = stl10_dataset
        self.weak_transform = weak_transform
        self.strong_transform = strong_transform
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ç»åº”ç”¨äº†è½¬æ¢
        self._check_transform()
    
    def _check_transform(self):
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ç»åº”ç”¨äº†è½¬æ¢"""
        sample, _ = self.dataset[0]
        if isinstance(sample, torch.Tensor):
            print("âš ï¸  è­¦å‘Š: æ— æ ‡ç­¾æ•°æ®é›†å·²ç»è¿”å›Tensorï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹å˜æ¢")
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        # STL-10 æ— æ ‡ç­¾æ•°æ®è¿”å› (image, -1)
        image, _ = self.dataset[idx]
        
        # ç¡®ä¿å›¾åƒæ˜¯PIL Imageï¼Œè€Œä¸æ˜¯Tensor
        if isinstance(image, torch.Tensor):
            # å¦‚æœæ˜¯Tensorï¼Œå…ˆè½¬æ¢ä¸ºPIL Image
            from torchvision.transforms.functional import to_pil_image
            image = to_pil_image(image)
        
        # åº”ç”¨ä¸¤ç§å¢å¼º
        weak_aug = self.weak_transform(image)
        strong_aug = self.strong_transform(image)
        
        # è¿”å›ä¸¤ç§å¢å¼ºç‰ˆæœ¬
        return (weak_aug, strong_aug), -1  # -1 è¡¨ç¤ºæ— æ ‡ç­¾
    
class STL10DataLoader:
    """
    STL-10 æ•°æ®åŠ è½½å™¨ç±»ï¼Œæ”¯æŒ Mean Teacher åŠç›‘ç£å­¦ä¹ 
    """
    
    def __init__(self, data_dir='./data', batch_size=32, validation_split=0.1):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.validation_split = validation_split
        
        # è‡ªåŠ¨è®¾ç½® num_workers
        if platform.system() == 'Windows':
            self.num_workers = 0
        else:
            self.num_workers = 4
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['OMP_NUM_THREADS'] = '1'
        
        # å®šä¹‰å˜æ¢
        self._define_transforms()
    
    def _define_transforms(self):
        """å®šä¹‰å„ç§æ•°æ®å˜æ¢"""
        
        # 1. åŸºç¡€å˜æ¢ï¼ˆæµ‹è¯•é›†ä½¿ç”¨ï¼‰
        self.base_transform = transforms.Compose([
            transforms.Resize((224, 224)),  # ResNetè¦æ±‚224x224
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # 2. å¼±å¢å¼ºå˜æ¢ï¼ˆç”¨äºæ•™å¸ˆæ¨¡å‹å’Œæœ‰æ ‡ç­¾è®­ç»ƒï¼‰
        self.weak_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomCrop(224, padding=4, padding_mode='reflect'),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # 3. å¼ºå¢å¼ºå˜æ¢ï¼ˆç”¨äºå­¦ç”Ÿæ¨¡å‹ï¼‰
        self.strong_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomCrop(224, padding=4, padding_mode='reflect'),
            transforms.ColorJitter(brightness=0.4, contrast=0.4, 
                                 saturation=0.4, hue=0.1),
            transforms.RandomGrayscale(p=0.2),
            transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.5, scale=(0.02, 0.1))
        ])
        
        # 4. æœ‰æ ‡ç­¾è®­ç»ƒå¢å¼ºï¼ˆä¸­ç­‰å¼ºåº¦ï¼‰
        self.labeled_train_transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomCrop(224, padding=8, padding_mode='reflect'),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, 
                                 saturation=0.2, hue=0.05),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def _check_dataset_sizes(self):
        """æ£€æŸ¥æ•°æ®é›†å¤§å°æ˜¯å¦æ­£ç¡®"""
        print("\nğŸ“Š STL-10 æ•°æ®é›†ä¿¡æ¯:")
        print("-" * 40)
        
        # æ£€æŸ¥æœ‰æ ‡ç­¾è®­ç»ƒé›†
        try:
            train_dataset_temp = datasets.STL10(
                root=self.data_dir,
                split='train',
                download=False,
                transform=None
            )
            print(f"æœ‰æ ‡ç­¾è®­ç»ƒé›†: {len(train_dataset_temp)} å¼ å›¾ç‰‡")
        except:
            print("æœ‰æ ‡ç­¾è®­ç»ƒé›†: æœªä¸‹è½½æˆ–è·¯å¾„é”™è¯¯")
        
        # æ£€æŸ¥æ— æ ‡ç­¾æ•°æ®
        try:
            unlabeled_dataset_temp = datasets.STL10(
                root=self.data_dir,
                split='unlabeled',
                download=False,
                transform=None
            )
            print(f"æ— æ ‡ç­¾æ•°æ®é›†: {len(unlabeled_dataset_temp)} å¼ å›¾ç‰‡")
        except:
            print("æ— æ ‡ç­¾æ•°æ®é›†: æœªä¸‹è½½æˆ–è·¯å¾„é”™è¯¯")
        
        # æ£€æŸ¥æµ‹è¯•é›†
        try:
            test_dataset_temp = datasets.STL10(
                root=self.data_dir,
                split='test',
                download=False,
                transform=None
            )
            print(f"æµ‹è¯•é›†: {len(test_dataset_temp)} å¼ å›¾ç‰‡")
        except:
            print("æµ‹è¯•é›†: æœªä¸‹è½½æˆ–è·¯å¾„é”™è¯¯")
        
        print("-" * 40)
    
    def get_mean_teacher_dataloaders(self, include_unlabeled=True):
        """
        ä¸º Mean Teacher è®­ç»ƒè·å–æ•°æ®åŠ è½½å™¨
        
        è¿”å›:
            labeled_train_loader: æœ‰æ ‡ç­¾è®­ç»ƒæ•°æ®
            val_loader: éªŒè¯æ•°æ®
            test_loader: æµ‹è¯•æ•°æ®
            unlabeled_loader: æ— æ ‡ç­¾æ•°æ®ï¼ˆMean Teacher æ ¼å¼ï¼‰
        """
        
        self._check_dataset_sizes()
        
        print("\nğŸš€ å‡†å¤‡ Mean Teacher æ•°æ®åŠ è½½å™¨...")
        
        # 1. æœ‰æ ‡ç­¾è®­ç»ƒé›†ï¼ˆä½¿ç”¨å¼±å¢å¼ºï¼‰
        print("åŠ è½½æœ‰æ ‡ç­¾è®­ç»ƒé›†...")
        labeled_train_dataset = datasets.STL10(
            root=self.data_dir,
            split='train',
            download=True,
            transform=self.labeled_train_transform
        )
        
        # éªŒè¯é›†ä»æœ‰æ ‡ç­¾æ•°æ®ä¸­åˆ’åˆ†
        val_size = int(self.validation_split * len(labeled_train_dataset))
        train_size = len(labeled_train_dataset) - val_size
        
        print(f"æœ‰æ ‡ç­¾æ•°æ®åˆ’åˆ†: {train_size} è®­ç»ƒ, {val_size} éªŒè¯")
        
        # éšæœºåˆ’åˆ†
        indices = torch.randperm(len(labeled_train_dataset)).tolist()
        train_indices = indices[:train_size]
        val_indices = indices[train_size:train_size + val_size]
        
        train_subset = Subset(labeled_train_dataset, train_indices)
        val_subset = Subset(labeled_train_dataset, val_indices)
        
        # 2. æµ‹è¯•é›†ï¼ˆä½¿ç”¨åŸºç¡€å˜æ¢ï¼‰
        print("åŠ è½½æµ‹è¯•é›†...")
        test_dataset = datasets.STL10(
            root=self.data_dir,
            split='test',
            download=True,
            transform=self.base_transform
        )
        
        # 3. æ— æ ‡ç­¾æ•°æ®é›†ï¼ˆMean Teacher æ ¼å¼ï¼‰
        unlabeled_loader = None
        if include_unlabeled:
            print("åŠ è½½æ— æ ‡ç­¾æ•°æ®é›†ï¼ˆMean Teacher æ ¼å¼ï¼‰...")
            
            # åŠ è½½åŸå§‹æ— æ ‡ç­¾æ•°æ®
            raw_unlabeled_dataset = datasets.STL10(
                root=self.data_dir,
                split='unlabeled',
                download=True,
                transform=transforms.ToTensor()  # åªè½¬æ¢ä¸ºtensorï¼Œåé¢åº”ç”¨å¢å¼º
            )
            
            # åˆ›å»º Mean Teacher ä¸“ç”¨æ•°æ®é›†
            mt_unlabeled_dataset = MeanTeacherUnlabeledDataset(
                raw_unlabeled_dataset,
                self.weak_transform,
                self.strong_transform
            )
            
            print(f"æ— æ ‡ç­¾æ•°æ®: {len(mt_unlabeled_dataset)} å¼ å›¾ç‰‡")
            print("  æ¯ä¸ªæ ·æœ¬æä¾›: å¼±å¢å¼ºï¼ˆæ•™å¸ˆï¼‰ + å¼ºå¢å¼ºï¼ˆå­¦ç”Ÿï¼‰")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        labeled_train_loader = DataLoader(
            train_subset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
            drop_last=True  # Mean Teacher ä¸­å»ºè®® drop_last
        )
        
        val_loader = DataLoader(
            val_subset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )
        
        if include_unlabeled:
            unlabeled_loader = DataLoader(
                mt_unlabeled_dataset,
                batch_size=self.batch_size,
                shuffle=True,
                num_workers=self.num_workers,
                pin_memory=True,
                drop_last=True
            )
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\nâœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        print(f"   æœ‰æ ‡ç­¾è®­ç»ƒé›†: {len(labeled_train_loader)} æ‰¹æ¬¡ Ã— {self.batch_size}")
        print(f"   éªŒè¯é›†: {len(val_loader)} æ‰¹æ¬¡")
        print(f"   æµ‹è¯•é›†: {len(test_loader)} æ‰¹æ¬¡")
        if unlabeled_loader:
            print(f"   æ— æ ‡ç­¾æ•°æ®: {len(unlabeled_loader)} æ‰¹æ¬¡ Ã— {self.batch_size}")
        
        return labeled_train_loader, val_loader, test_loader, unlabeled_loader
    
    def get_class_distribution(self):
        """è·å–æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡åˆ†å¸ƒ"""
        
        # åŠ è½½æœ‰æ ‡ç­¾è®­ç»ƒé›†
        train_dataset = datasets.STL10(
            root=self.data_dir,
            split='train',
            download=True,
            transform=None
        )
        
        # STL-10 ç±»åˆ«åç§°
        class_names = ['airplane', 'bird', 'car', 'cat', 'deer',
                      'dog', 'horse', 'monkey', 'ship', 'truck']
        
        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡
        class_counts = {name: 0 for name in class_names}
        
        for _, label in train_dataset:
            class_counts[class_names[label]] += 1
        
        print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
        print("-" * 40)
        for class_name, count in class_counts.items():
            percentage = (count / len(train_dataset)) * 100
            print(f"{class_name:10s}: {count:4d} å¼  ({percentage:5.1f}%)")
        
        return class_counts


def visualize_augmentations(data_loader, num_samples=3):
    """
    å¯è§†åŒ–æ•°æ®å¢å¼ºæ•ˆæœ
    """
    import matplotlib.pyplot as plt
    
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    for batch in data_loader:
        if isinstance(batch, tuple) and len(batch) == 2:
            (weak_augs, strong_augs), _ = batch
            break
    
    fig, axes = plt.subplots(num_samples, 2, figsize=(10, num_samples * 4))
    
    for i in range(num_samples):
        # è½¬æ¢ä¸ºå¯æ˜¾ç¤ºçš„æ ¼å¼
        weak_img = weak_augs[i].cpu().numpy().transpose(1, 2, 0)
        weak_img = weak_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
        weak_img = np.clip(weak_img, 0, 1)
        
        strong_img = strong_augs[i].cpu().numpy().transpose(1, 2, 0)
        strong_img = strong_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
        strong_img = np.clip(strong_img, 0, 1)
        
        # æ˜¾ç¤ºå›¾åƒ
        axes[i, 0].imshow(weak_img)
        axes[i, 0].set_title(f"æ ·æœ¬ {i+1}: å¼±å¢å¼º")
        axes[i, 0].axis('off')
        
        axes[i, 1].imshow(strong_img)
        axes[i, 1].set_title(f"æ ·æœ¬ {i+1}: å¼ºå¢å¼º")
        axes[i, 1].axis('off')
    
    plt.suptitle("Mean Teacher æ•°æ®å¢å¼ºå¯¹æ¯”", fontsize=16)
    plt.tight_layout()
    plt.savefig('mean_teacher_augmentations.png', dpi=150, bbox_inches='tight')
    plt.show()


# å‘åå…¼å®¹çš„å‡½æ•°
def get_stl10_dataloaders(batch_size=32, use_resnet_preprocessing=True, 
                         validation_split=0.1, include_unlabeled=True, 
                         num_workers=None):
    """
    å‘åå…¼å®¹çš„å‡½æ•°ï¼Œä½¿ç”¨æ–°çš„æ•°æ®åŠ è½½å™¨
    """
    # å¦‚æœæŒ‡å®šäº†num_workersï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„å€¼
    if num_workers is not None:
        import warnings
        warnings.warn("num_workerså‚æ•°å°†åœ¨æ–°ç‰ˆæœ¬ä¸­è¢«å¿½ç•¥ï¼Œè¯·ä½¿ç”¨STL10DataLoaderç±»")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨å®ä¾‹
    loader = STL10DataLoader(
        data_dir='./data',
        batch_size=batch_size,
        validation_split=validation_split
    )
    
    return loader.get_mean_teacher_dataloaders(include_unlabeled=include_unlabeled)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• Mean Teacher æ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = STL10DataLoader(
        data_dir='./data',
        batch_size=16,  # ä½¿ç”¨å°æ‰¹æ¬¡ä¾¿äºæµ‹è¯•
        validation_split=0.1
    )
    
    # è·å–ç±»åˆ«åˆ†å¸ƒ
    class_dist = loader.get_class_distribution()
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader, unlabeled_loader = loader.get_mean_teacher_dataloaders()
    
    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    print("\nğŸ” æ£€æŸ¥æ•°æ®å½¢çŠ¶:")
    
    # æœ‰æ ‡ç­¾æ•°æ®
    for images, labels in train_loader:
        print(f"æœ‰æ ‡ç­¾è®­ç»ƒæ•°æ®å½¢çŠ¶: {images.shape}")
        print(f"æœ‰æ ‡ç­¾è®­ç»ƒæ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        print(f"æ ‡ç­¾å€¼ç¤ºä¾‹: {labels[:5].numpy()}")
        break
    
    # æ— æ ‡ç­¾æ•°æ®
    if unlabeled_loader:
        for (weak_augs, strong_augs), _ in unlabeled_loader:
            print(f"æ— æ ‡ç­¾å¼±å¢å¼ºå½¢çŠ¶: {weak_augs.shape}")
            print(f"æ— æ ‡ç­¾å¼ºå¢å¼ºå½¢çŠ¶: {strong_augs.shape}")
            break
    
    # éªŒè¯æ•°æ®
    for images, labels in val_loader:
        print(f"éªŒè¯æ•°æ®å½¢çŠ¶: {images.shape}")
        print(f"éªŒè¯æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        break
    
    print("\nâœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ!")
    
    # å¯è§†åŒ–å¢å¼ºæ•ˆæœï¼ˆå¯é€‰ï¼‰
    # if unlabeled_loader:
    #     visualize_augmentations(unlabeled_loader)
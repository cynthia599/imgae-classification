# utils.py
import torch
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

def plot_mean_teacher_results(results):
    """ç»˜åˆ¶Mean Teacherè®­ç»ƒç»“æœ"""
    # ====== æ·»åŠ æ ‡ç­¾æ˜ å°„ä¿®å¤æ˜¾ç¤ºé—®é¢˜ ======
    # ä¿®å¤å­—ä½“å¯¼è‡´çš„æ˜¾ç¤ºé—®é¢˜ï¼ˆSæ˜¾ç¤ºä¸º5ï¼‰
    label_mapping = {
        'MeanTeacher_SGD': 'MeanTeacher_SGD',
        'MeanTeacher_Adam': 'MeanTeacher_Adam',
        'MeanTeacher_AdamW': 'MeanTeacher_AdamW',
        # æ·»åŠ å…¶ä»–å¯èƒ½çš„å®éªŒåç§°
    }
    
    # åˆ›å»ºæ˜¾ç¤ºåç§°åˆ—è¡¨
    exp_names = list(results.keys())
    display_names = [label_mapping.get(name, name) for name in exp_names]
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # ç»˜åˆ¶æ€»æŸå¤± - ä½¿ç”¨display_namesè€Œä¸æ˜¯exp_names
    for exp_name, display_name in zip(exp_names, display_names):
        ax1.plot(results[exp_name]['train_losses'], label=display_name, linewidth=2)
    
    ax1.set_title('Mean Teacher Total Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶ç›‘ç£æŸå¤±
    for exp_name, display_name in zip(exp_names, display_names):
        if 'supervised_losses' in results[exp_name]:
            ax2.plot(results[exp_name]['supervised_losses'], label=display_name, linewidth=2)
    
    ax2.set_title('Supervised Loss')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶ä¸€è‡´æ€§æŸå¤±
    for exp_name, result in results.items():
        if 'consistency_losses' in result:
            ax3.plot(result['consistency_losses'], label=exp_name, linewidth=2)
    
    ax3.set_title('Consistency Loss')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Loss')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡
    for exp_name, result in results.items():
        ax4.plot(result['val_accuracies'], label=exp_name, linewidth=2)
    
    ax4.set_title('Validation Accuracy')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('Accuracy (%)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('mean_teacher_training_curves.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç»˜åˆ¶æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ
    fig, ax = plt.subplots(figsize=(10, 6))
    exp_names = list(results.keys())
    test_accuracies = [results[name]['final_accuracy'] for name in exp_names]
    
    x = np.arange(len(exp_names))
    
    ax.bar(x, test_accuracies, alpha=0.8, color=['blue', 'orange', 'green'])
    
    ax.set_xlabel('Mean Teacher Configuration')
    ax.set_ylabel('Test Accuracy (%)')
    ax.set_title('Mean Teacher Final Performance Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(exp_names, rotation=45)
    ax.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(test_accuracies):
        ax.text(i, v + 0.5, f'{v:.2f}%', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('mean_teacher_final_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
def evaluate_model(model, test_loader, device):
    """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
    model.eval()
    correct = 0
    total = 0
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    accuracy = 100 * correct / total
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_correct = [0] * 10
    class_total = [0] * 10
    
    for i in range(len(all_labels)):
        label = all_labels[i]
        class_correct[label] += (all_predictions[i] == label)
        class_total[label] += 1
    
    class_accuracy = [100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0 
                     for i in range(10)]
    
    return accuracy, class_accuracy

def calculate_loss(model, data_loader, criterion, device):
    """è®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°æ®åŠ è½½å™¨ä¸Šçš„æŸå¤±"""
    model.eval()
    running_loss = 0.0
    total_samples = 0
    
    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * images.size(0)
            total_samples += images.size(0)
    
    avg_loss = running_loss / total_samples
    return avg_loss

def plot_optimizer_comparison(results):
    """ç»˜åˆ¶ä¸åŒä¼˜åŒ–å™¨çš„è®­ç»ƒå†å²æ¯”è¾ƒå›¾"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç»˜åˆ¶è®­ç»ƒæŸå¤±
    for exp_name, result in results.items():
        ax1.plot(result['train_losses'], label=exp_name, linewidth=2)
    
    ax1.set_title('Training Loss Comparison')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡
    for exp_name, result in results.items():
        ax2.plot(result['val_accuracies'], label=exp_name, linewidth=2)
    
    ax2.set_title('Validation Accuracy Comparison')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('optimizer_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç»˜åˆ¶æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ
    fig, ax = plt.subplots(figsize=(10, 6))
    exp_names = list(results.keys())
    test_accuracies = [results[name]['final_accuracy'] for name in exp_names]
    val_accuracies = [results[name]['val_accuracies'][-1] for name in exp_names]
    
    x = np.arange(len(exp_names))
    width = 0.35
    
    ax.bar(x - width/2, test_accuracies, width, label='Test Accuracy', alpha=0.8)
    ax.bar(x + width/2, val_accuracies, width, label='Validation Accuracy', alpha=0.8)
    
    ax.set_xlabel('Optimizer')
    ax.set_ylabel('Accuracy (%)')
    ax.set_title('Final Performance Comparison')
    ax.set_xticks(x)
    ax.set_xticklabels(exp_names, rotation=45)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(test_accuracies):
        ax.text(i - width/2, v + 0.5, f'{v:.1f}%', ha='center', va='bottom')
    for i, v in enumerate(val_accuracies):
        ax.text(i + width/2, v + 0.5, f'{v:.1f}%', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('final_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# æ³¨é‡Šæ‰äº†plot_rmsprop_comparisonå‡½æ•°ï¼Œå› ä¸ºå®ƒåŒ…å«é’ˆå¯¹ä¸åŒå‚æ•°è®¾ç½®çš„RMSpropä¼˜åŒ–å™¨çš„ä»£ç 
# def plot_rmsprop_comparison(results):
#     """ä¸“é—¨ç»˜åˆ¶RMSpropå‚æ•°ä¼˜åŒ–çš„æ¯”è¾ƒå›¾"""
#     # åˆ›å»º2x2çš„å­å›¾å¸ƒå±€
#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
#     # ç»˜åˆ¶è®­ç»ƒæŸå¤±
#     for exp_name, result in results.items():
#         ax1.plot(result['train_losses'], label=exp_name, linewidth=2)
    
#     ax1.set_title('RMSprop Training Loss Comparison')
#     ax1.set_xlabel('Epoch')
#     ax1.set_ylabel('Loss')
#     ax1.legend()
#     ax1.grid(True, alpha=0.3)
    
#     # ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡
#     for exp_name, result in results.items():
#         ax2.plot(result['val_accuracies'], label=exp_name, linewidth=2)
    
#     ax2.set_title('RMSprop Validation Accuracy Comparison')
#     ax2.set_xlabel('Epoch')
#     ax2.set_ylabel('Accuracy (%)')
#     ax2.legend()
#     ax2.grid(True, alpha=0.3)
    
#     # ç»˜åˆ¶è®­ç»ƒvséªŒè¯æŸå¤±
#     for exp_name, result in results.items():
#         if 'val_losses' in result and result['val_losses']:
#             epochs = range(1, len(result['train_losses']) + 1)
#             ax3.plot(epochs, result['train_losses'], label=f'{exp_name} (Train)', linewidth=1.5, linestyle='-')
#             ax3.plot(epochs, result['val_losses'], label=f'{exp_name} (Val)', linewidth=1.5, linestyle='--')
    
#     ax3.set_title('RMSprop Train vs Validation Loss')
#     ax3.set_xlabel('Epoch')
#     ax3.set_ylabel('Loss')
#     ax3.legend(fontsize='small')
#     ax3.grid(True, alpha=0.3)
    
#     # ç»˜åˆ¶æœ€ç»ˆæ€§èƒ½æ¯”è¾ƒ
#     exp_names = list(results.keys())
#     test_accuracies = [results[name]['final_accuracy'] for name in exp_names]
#     val_accuracies = [results[name]['val_accuracies'][-1] for name in exp_names]
    
#     x = np.arange(len(exp_names))
#     width = 0.35
    
#     ax4.bar(x - width/2, test_accuracies, width, label='Test Accuracy', alpha=0.8)
#     ax4.bar(x + width/2, val_accuracies, width, label='Validation Accuracy', alpha=0.8)
    
#     ax4.set_xlabel('RMSprop Configuration')
#     ax4.set_ylabel('Accuracy (%)')
#     ax4.set_title('RMSprop Final Performance Comparison')
#     ax4.set_xticks(x)
#     ax4.set_xticklabels(exp_names, rotation=45, ha='right')
#     ax4.legend()
#     ax4.grid(True, alpha=0.3)
    
#     # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
#     for i, v in enumerate(test_accuracies):
#         ax4.text(i - width/2, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontsize=8)
#     for i, v in enumerate(val_accuracies):
#         ax4.text(i + width/2, v + 0.5, f'{v:.1f}%', ha='center', va='bottom', fontsize=8)
    
#     plt.tight_layout()
#     plt.savefig('rmsprop_optimization_comparison.png', dpi=300, bbox_inches='tight')
#     plt.show()
    
#     # ç»˜åˆ¶å­¦ä¹ ç‡å¯¹æ¯”
#     fig, ax = plt.subplots(figsize=(12, 6))
    
#     # æå–å­¦ä¹ ç‡ä¿¡æ¯
#     lr_values = []
#     config_names = []
#     for exp_name in exp_names:
#         if 'VeryLow' in exp_name:
#             lr = 0.00002
#         elif 'Low' in exp_name:
#             lr = 0.00005
#         elif 'Medium' in exp_name:
#             lr = 0.0001
#         else:
#             lr = 0.00005  # é»˜è®¤
        
#         lr_values.append(lr)
#         config_names.append(exp_name)
    
#     # ç»˜åˆ¶å­¦ä¹ ç‡ä¸å‡†ç¡®ç‡çš„å…³ç³»
#     scatter = ax.scatter(lr_values, test_accuracies, c=val_accuracies, cmap='viridis', s=100, alpha=0.7)
#     ax.set_xlabel('Learning Rate')
#     ax.set_ylabel('Test Accuracy (%)')
#     ax.set_title('RMSprop: Learning Rate vs Test Accuracy')
#     ax.set_xscale('log')
#     ax.grid(True, alpha=0.3)
    
#     # æ·»åŠ é¢œè‰²æ¡è¡¨ç¤ºéªŒè¯å‡†ç¡®ç‡
#     cbar = plt.colorbar(scatter)
#     cbar.set_label('Validation Accuracy (%)')
    
#     # æ·»åŠ é…ç½®åç§°æ ‡ç­¾
#     for i, name in enumerate(config_names):
#         ax.annotate(name, (lr_values[i], test_accuracies[i]), 
#                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
#     plt.tight_layout()
#     plt.savefig('rmsprop_lr_vs_accuracy.png', dpi=300, bbox_inches='tight')
#     plt.show()

def print_experiment_summary(results):
    """æ‰“å°å®éªŒæ€»ç»“"""
    print("\n" + "="*80)
    print("å®éªŒæ€»ç»“")
    print("="*80)
    
    # æŒ‰æµ‹è¯•å‡†ç¡®ç‡æ’åº
    sorted_results = sorted(results.items(), key=lambda x: x[1]['final_accuracy'], reverse=True)
    
    print(f"{'ä¼˜åŒ–å™¨':<25} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'éªŒè¯å‡†ç¡®ç‡':<12} {'è®­ç»ƒè½®æ•°':<10}")
    print("-"*80)
    
    for exp_name, result in sorted_results:
        test_acc = result['final_accuracy']
        val_acc = result['val_accuracies'][-1] if result['val_accuracies'] else 0
        epochs = len(result['train_losses'])
        
        print(f"{exp_name:<25} {test_acc:<11.2f}% {val_acc:<11.2f}% {epochs:<10}")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_exp = max(results.items(), key=lambda x: x[1]['final_accuracy'])
    print("\nğŸ‰ æœ€ä½³æ¨¡å‹:")
    print(f"   åç§°: {best_exp[0]}")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_exp[1]['final_accuracy']:.2f}%")
    print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {best_exp[1]['val_accuracies'][-1]:.2f}%")

def check_overfitting(results, threshold=0.5):
    """æ£€æŸ¥è¿‡æ‹Ÿåˆæƒ…å†µ
    Args:
        results: è®­ç»ƒç»“æœå­—å…¸
        threshold: è¿‡æ‹Ÿåˆé˜ˆå€¼ï¼Œæµ‹è¯•é›†ä¸éªŒè¯é›†å‡†ç¡®ç‡å·®è·è¶…è¿‡æ­¤å€¼è®¤ä¸ºå¯èƒ½è¿‡æ‹Ÿåˆ
    """
    print("\n" + "="*50)
    print("è¿‡æ‹Ÿåˆåˆ†æ")
    print("="*50)
    
    for exp_name, result in results.items():
        if len(result['val_accuracies']) > 0:
            train_loss_final = result['train_losses'][-1] if result['train_losses'] else 0
            val_acc_final = result['val_accuracies'][-1]
            test_acc = result['final_accuracy']
            
            # è®¡ç®—æµ‹è¯•é›†ä¸éªŒè¯é›†å‡†ç¡®ç‡çš„å·®è·
            gap = test_acc - val_acc_final
            
            print(f"{exp_name}:")
            print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss_final:.4f}")
            if 'val_losses' in result and result['val_losses']:
                print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {result['val_losses'][-1]:.4f}")
            if 'final_test_loss' in result:
                print(f"  æœ€ç»ˆæµ‹è¯•æŸå¤±: {result['final_test_loss']:.4f}")
            print(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_acc_final:.2f}%")
            print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
            print(f"  æµ‹è¯•-éªŒè¯å·®è·: {gap:+.2f}%")
            
            if gap > threshold:
                print(f"  âš ï¸  å¯èƒ½è¿‡æ‹Ÿåˆ (å·®è· > {threshold}%)")
            elif gap < -threshold:
                print(f"  ğŸ” å¯èƒ½æ¬ æ‹Ÿåˆ (å·®è· < -{threshold}%)")
            else:
                print(f"  âœ… æ‹Ÿåˆè‰¯å¥½")
            print()

def plot_confusion_matrix(model, test_loader, device, class_names=None):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    if class_names is None:
        class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 
                      'dog', 'horse', 'monkey', 'ship', 'truck']
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_predictions)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return cm

def print_detailed_classification_report(model, test_loader, device, class_names=None):
    """æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š"""
    if class_names is None:
        class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 
                      'dog', 'horse', 'monkey', 'ship', 'truck']
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(all_labels, all_predictions, 
                                  target_names=class_names, digits=4)
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print("="*60)
    print(report)
    
    return report

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("Utilsæ¨¡å—æµ‹è¯•")
    
    # æ¨¡æ‹Ÿä¸€äº›ç»“æœæ•°æ®ç”¨äºæµ‹è¯•ç»˜å›¾å‡½æ•°
    test_results = {
        'MeanTeacher_Adam': {
            'train_losses': [2.1, 1.5, 1.2, 0.9, 0.7, 0.5, 0.4, 0.3],
            'val_accuracies': [45.2, 58.7, 65.3, 68.9, 72.1, 75.3, 77.8, 79.2],
            'supervised_losses': [2.0, 1.6, 1.3, 1.0, 0.8, 0.6, 0.5, 0.4],
            'consistency_losses': [0.1, 0.08, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01],
            'final_accuracy': 79.5,
        },
        'MeanTeacher_SGD': {
            'train_losses': [2.3, 1.8, 1.4, 1.1, 0.8, 0.6, 0.45, 0.35],
            'val_accuracies': [42.1, 53.4, 61.2, 66.8, 69.5, 72.8, 75.6, 77.9],
            'supervised_losses': [2.2, 1.9, 1.5, 1.2, 0.9, 0.7, 0.55, 0.45],
            'consistency_losses': [0.15, 0.12, 0.09, 0.07, 0.06, 0.05, 0.04, 0.03],
            'final_accuracy': 78.2,
        },
        'MeanTeacher_AdamW': {
            'train_losses': [2.0, 1.4, 1.0, 0.7, 0.5, 0.35, 0.25, 0.2],
            'val_accuracies': [48.5, 62.3, 70.1, 74.8, 77.2, 79.1, 80.5, 81.2],
            'supervised_losses': [1.9, 1.5, 1.1, 0.8, 0.6, 0.45, 0.35, 0.3],
            'consistency_losses': [0.08, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.01],
            'final_accuracy': 81.5,
        }
    }
    
    # æµ‹è¯•Mean Teacheræ¯”è¾ƒå‡½æ•°
    plot_mean_teacher_results(test_results)
    print_experiment_summary(test_results)
    check_overfitting(test_results)
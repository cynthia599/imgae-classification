# train_optimized_gpu.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from data_loader import get_stl10_dataloaders
from model import STL10_ResNet18
from utils import evaluate_model, plot_optimizer_comparison, print_experiment_summary
import pickle
import time

def calculate_loss(model, data_loader, criterion, device):
    """è®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°æ®åŠ è½½å™¨ä¸Šçš„æŸå¤±"""
    model.eval()
    running_loss = 0.0
    total_samples = 0
    
    with torch.no_grad():
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * images.size(0)
            total_samples += images.size(0)
    
    avg_loss = running_loss / total_samples
    return avg_loss

def setup_device():
    """è®¾ç½®è®¾å¤‡å¹¶ä¼˜åŒ–GPUæ€§èƒ½"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        
        # GPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = True  # åŠ é€Ÿå·ç§¯è¿ç®—
        torch.backends.cudnn.deterministic = False  # ä¸ºäº†é€Ÿåº¦ç‰ºç‰²å¯é‡å¤æ€§
        
        gpu_props = torch.cuda.get_device_properties(0)
        print(f"âœ… ä½¿ç”¨GPU: {gpu_props.name}")
        print(f"   GPUå†…å­˜: {gpu_props.total_memory / 1024**3:.1f} GB")
        print(f"   CUDAæ ¸å¿ƒ: {gpu_props.multi_processor_count}")
        
    else:
        device = torch.device('cpu')
        print("âš ï¸ ä½¿ç”¨CPU (GPUä¸å¯ç”¨)")
    
    return device

def train_single_experiment(optimizer_name='adam', learning_rate=0.001, batch_size=64, epochs=15, 
                           use_amp=True, model_type='resnet18', validation_split=0.1):
    """ä¼˜åŒ–çš„GPUè®­ç»ƒå®éªŒ"""
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device()
    
    # è·å–æ•°æ® - åŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    train_loader, val_loader, test_loader = get_stl10_dataloaders(
        batch_size=batch_size, 
        use_resnet_preprocessing=True,
        validation_split=validation_split
    )
    
    # é€‰æ‹©æ¨¡å‹
    if model_type == 'resnet18':
        model = STL10_ResNet18(pretrained=True, feature_extract=False)
        print("ä½¿ç”¨ResNet18æ¨¡å‹ (å®Œæ•´å¾®è°ƒ)")
    else:
        from model import SimpleCNN
        model = SimpleCNN()
        print("ä½¿ç”¨ç®€å•CNNæ¨¡å‹")
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°: {device}")
    
    # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ (AMP) - å¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜ä½¿ç”¨
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
    
    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    
   # ä¼˜åŒ–å™¨é…ç½® - å¢åŠ æƒé‡è¡°å‡ï¼ˆæ›´å¼ºçš„L2æ­£åˆ™åŒ–ï¼‰
    weight_decay = 1e-3
    
    if optimizer_name.lower() == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'rmsprop':
        optimizer = optim.RMSprop(
            model.parameters(), 
            lr=learning_rate, 
            alpha=0.99,
            momentum=0.9,
            weight_decay=weight_decay,
            eps=1e-8
        )
    
    # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ›´é€‚åˆé•¿è®­ç»ƒ
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    
    print(f"\nå¼€å§‹è®­ç»ƒ {optimizer_name} ä¼˜åŒ–å™¨...")
    print(f"æ¨¡å‹: {model_type}")
    print(f"æ··åˆç²¾åº¦è®­ç»ƒ: {use_amp}")
    print(f"è®¾å¤‡: {device}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print(f"éªŒè¯é›†æ¯”ä¾‹: {validation_split}")
    print(f"æ•°æ®é›†: STL-10")
    
    # è®°å½•è®­ç»ƒè¿‡ç¨‹
    train_losses = []
    val_accuracies = []
    val_losses = []  # æ–°å¢ï¼šéªŒè¯é›†loss
    test_losses = []  # æ–°å¢ï¼šæµ‹è¯•é›†lossï¼ˆå®šæœŸè®¡ç®—ï¼‰
    training_times = []
    
    # æ—©åœå‚æ•°
    # best_val_accuracy = 0
    # patience = 5
    # patience_counter = 0
    # best_model_state = None

    # åœ¨è®­ç»ƒå¾ªç¯å¼€å§‹å‰æ·»åŠ çƒ­èº«
    warmup_epochs = 3
    for epoch in range(epochs):
        # å­¦ä¹ ç‡çƒ­èº«
        if epoch < warmup_epochs:
            lr_scale = (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = learning_rate * lr_scale
        
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        epoch_start_time = time.time()
        model.train()
        running_loss = 0.0

        # ========== Warmupå­¦ä¹ ç‡è°ƒæ•´ ==========
        if epoch < warmup_epochs:
            # çº¿æ€§å¢åŠ å­¦ä¹ ç‡: ä»0.1*lrå¢åŠ åˆ°ç›®æ ‡lr
            warmup_lr = learning_rate * (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = warmup_lr
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.amp.autocast('cuda', enabled=use_amp):
                outputs = model(images)
                loss = criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            running_loss += loss.item()
            
            # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦
            if batch_idx % 50 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f'Epoch: {epoch+1}/{epochs} | Batch: {batch_idx}/{len(train_loader)} | '
                      f'Loss: {loss.item():.4f} | LR: {current_lr:.6f}')
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = running_loss / len(train_loader)
        train_losses.append(avg_loss)
        
        # æ›´æ–°å­¦ä¹ ç‡
        if epoch >= warmup_epochs:
            scheduler.step()
        
        # æ¯ä¸ªepochç»“æŸååœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        accuracy, _ = evaluate_model(model, val_loader, device)
        val_accuracies.append(accuracy)

        # æ–°å¢ï¼šè®¡ç®—éªŒè¯é›†loss
        val_loss = calculate_loss(model, val_loader, criterion, device)
        val_losses.append(val_loss)

        # æ¯5ä¸ªepochè®¡ç®—ä¸€æ¬¡æµ‹è¯•é›†loss
        if epoch % 5 == 0 or epoch == epochs - 1:
            test_loss = calculate_loss(model, test_loader, criterion, device)
            test_losses.append({'epoch': epoch, 'loss': test_loss})
        
        epoch_time = time.time() - epoch_start_time
        training_times.append(epoch_time)
        
        warmup_status = "[Warmup]" if epoch < warmup_epochs else ""
        print(f'Epoch [{epoch+1}/{epochs}] | æ—¶é—´: {epoch_time:.1f}s | '
              f'å¹³å‡æŸå¤±: {avg_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}% {warmup_status}')
        # # æ—©åœé€»è¾‘
        # if accuracy > best_val_accuracy:
        #     best_val_accuracy = accuracy
        #     patience_counter = 0
        #     # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
        #     best_model_state = model.state_dict().copy()
        #     print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}%")
        # else:
        #     patience_counter += 1
        #     print(f"âš ï¸  éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{patience}")
            
        # if patience_counter >= patience:
        #     print(f"ğŸ›‘ æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
        #     # æ¢å¤æœ€ä½³æ¨¡å‹
        #     if best_model_state is not None:
        #         model.load_state_dict(best_model_state)
        #     break
    
    # æœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    final_accuracy, class_accuracy = evaluate_model(model, test_loader, device)
    final_test_loss = calculate_loss(model, test_loader, criterion, device)
    avg_epoch_time = sum(training_times) / len(training_times)
    print(f"å¹³å‡æ¯è½®è®­ç»ƒæ—¶é—´: {avg_epoch_time:.1f}ç§’")
    print(f"æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡: {final_accuracy:.2f}%")
    print(f"æœ€ç»ˆæµ‹è¯•é›†æŸå¤±: {final_test_loss:.4f}")

    return {
        'model': model,
        'train_losses': train_losses,
        'val_accuracies': val_accuracies,
        'val_losses': val_losses,  # æ–°å¢
        'test_losses': test_losses,  # æ–°å¢
        'final_accuracy': final_accuracy,
        'class_accuracy': class_accuracy,
        'training_times': training_times,
        'final_test_loss': final_test_loss  # æ–°å¢
        # 'best_val_accuracy': best_val_accuracy
    }

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œå®Œæ•´çš„30ä¸ªepochè®­ç»ƒ"""
    
    print("STL-10 å›¾åƒåˆ†ç±»å®éªŒ - 100ä¸ªepochå®Œæ•´è®­ç»ƒ")
    print("Python 3.9 + PyTorch 2.7.1 + CUDA 12.8")
    print("="*60)
    
    # å®éªŒé…ç½® - 30ä¸ªepochï¼Œæ›´å¼ºçš„æ­£åˆ™åŒ–
    experiments = [
    {'name': 'ResNet18_Adam', 'optimizer': 'adam', 'lr': 0.0001, 'batch_size': 128, 'epochs': 30, 'val_split': 0.1},
    {'name': 'ResNet18_AdamW', 'optimizer': 'adamw', 'lr': 0.0001, 'batch_size': 128, 'epochs': 30, 'val_split': 0.1},
    {'name': 'ResNet18_SGD', 'optimizer': 'sgd', 'lr': 0.001, 'batch_size': 128, 'epochs': 30, 'val_split': 0.1},  # ä»0.01é™åˆ°0.001
    {'name': 'RMSprop_VeryLowLR_NoMomentum', 'optimizer': 'rmsprop', 'lr': 0.00002, 'batch_size': 128, 'epochs': 20, 'val_split': 0.1, 'momentum': 0.0},
    {'name': 'RMSprop_LowLR_NoMomentum', 'optimizer': 'rmsprop', 'lr': 0.00005, 'batch_size': 128, 'epochs': 20, 'val_split': 0.1, 'momentum': 0.0},
]
    
    results = {}
    
    for exp_config in experiments:
        print(f"\n{'='*50}")
        print(f"å®éªŒ: {exp_config['name']}")
        print(f"{'='*50}")
        
        result = train_single_experiment(
            optimizer_name=exp_config['optimizer'],
            learning_rate=exp_config['lr'],
            batch_size=exp_config['batch_size'],
            epochs=exp_config['epochs'],
            use_amp=True,  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            model_type='resnet18',
            validation_split=exp_config['val_split']
        )
        
        results[exp_config['name']] = result
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print(f"\nğŸ“Š {exp_config['name']} æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {result['final_accuracy']:.2f}%")
        # print(f"ğŸ† {exp_config['name']} æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {result['best_val_accuracy']:.2f}%")
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_optimizer_comparison(results)
    
    # æ¯”è¾ƒä¸åŒé…ç½®çš„æ€§èƒ½
    print_experiment_summary(results)
    
    # ä¿å­˜ç»“æœ
    with open('optimized_gpu_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    print("è®­ç»ƒç»“æœå·²ä¿å­˜åˆ° optimized_gpu_results.pkl")
    
    # æ€§èƒ½ç»Ÿè®¡
    total_training_time = sum(sum(result['training_times']) for result in results.values())
    print(f"\næ€»è®­ç»ƒæ—¶é—´: {total_training_time:.1f}ç§’ ({total_training_time/60:.1f}åˆ†é’Ÿ)")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_exp = max(results.items(), key=lambda x: x[1]['final_accuracy'])
    print(f"\nğŸ‰ æœ€ä½³æ¨¡å‹: {best_exp[0]} - æµ‹è¯•å‡†ç¡®ç‡: {best_exp[1]['final_accuracy']:.2f}%")
    
    return results

if __name__ == "__main__":
    results = main()
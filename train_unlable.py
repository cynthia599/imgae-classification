"""
STL-10 Mean Teacher åŠç›‘ç£å­¦ä¹ å®éªŒ
åŒ…å«å®Œæ•´çš„ç±»åˆ«åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
import platform
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.optim.lr_scheduler import CosineAnnealingLR
from data_loader import get_stl10_dataloaders
from model import STL10_ResNet18
from utils import evaluate_model, plot_mean_teacher_results, print_experiment_summary, check_overfitting, calculate_loss
import pickle
import time
import os
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
from PIL import Image

# ========== æ–°å¢åˆ†æå‡½æ•° ==========

def analyze_class_performance(model, test_loader, device, class_names=None):
    """
    åˆ†ææ¨¡å‹åœ¨æ¯ä¸ªç±»åˆ«ä¸Šçš„æ€§èƒ½
    """
    model.eval()
    
    # å¦‚æœæ²¡æœ‰æä¾›ç±»åˆ«åç§°ï¼Œä½¿ç”¨STL-10é»˜è®¤
    if class_names is None:
        class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 
                      'dog', 'horse', 'monkey', 'ship', 'truck']
    
    num_classes = len(class_names)
    class_correct = [0] * num_classes
    class_total = [0] * num_classes
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ•°
            c = (predicted == labels).squeeze()
            
            for i in range(len(labels)):
                label = labels[i].item()
                class_correct[label] += c[i].item()
                class_total[label] += 1
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_accuracies = {}
    for i in range(num_classes):
        if class_total[i] > 0:
            accuracy = 100 * class_correct[i] / class_total[i]
            class_accuracies[class_names[i]] = accuracy
    
    # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®ç±»åˆ«
    sorted_classes = sorted(class_accuracies.items(), 
                           key=lambda x: x[1], 
                           reverse=True)
    
    best_class = sorted_classes[0] if sorted_classes else ("None", 0)
    worst_class = sorted_classes[-1] if sorted_classes else ("None", 0)
    
    return class_accuracies, best_class, worst_class, None

def analyze_unlabeled_data_performance(teacher_model, unlabeled_loader, device, 
                                      confidence_threshold=0.8, num_samples=1000):
    """
    åˆ†ææ¨¡å‹åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„é¢„æµ‹ç½®ä¿¡åº¦
    """
    teacher_model.eval()
    
    confidence_scores = []
    predicted_classes = []
    sample_predictions = []
    
    # åªåˆ†æéƒ¨åˆ†æ ·æœ¬ï¼Œé¿å…å†…å­˜é—®é¢˜
    num_analyzed = 0
    
    with torch.no_grad():
        for batch in unlabeled_loader:
            if num_analyzed >= num_samples:
                break
            
            # æ ¹æ®æ‚¨çš„æ•°æ®ç»“æ„ï¼Œbatchæ˜¯åˆ—è¡¨ï¼š[images_list, labels]
            if isinstance(batch, list) and len(batch) == 2:
                images_list = batch[0]
                if isinstance(images_list, list) and len(images_list) >= 1:
                    # ä½¿ç”¨å¼±å¢å¼ºå›¾åƒè¿›è¡Œé¢„æµ‹
                    images = images_list[0].to(device)
                    outputs = teacher_model(images)
                    probabilities = torch.softmax(outputs, dim=1)
                    
                    # è·å–æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
                    max_probs, preds = torch.max(probabilities, dim=1)
                    
                    confidence_scores.extend(max_probs.cpu().numpy())
                    predicted_classes.extend(preds.cpu().numpy())
                    
                    # å­˜å‚¨é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„é¢„æµ‹
                    for i in range(len(images)):
                        if num_analyzed >= num_samples:
                            break
                        
                        sample_predictions.append({
                            'predicted_class': preds[i].item(),
                            'confidence': max_probs[i].item(),
                            'probabilities': probabilities[i].cpu().numpy()
                        })
                        num_analyzed += 1
    
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¬¡æ•°å’Œå¹³å‡ç½®ä¿¡åº¦
    class_stats = {}
    class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 
                  'dog', 'horse', 'monkey', 'ship', 'truck']
    
    for class_idx in range(10):
        # è·å–å±äºè¯¥ç±»åˆ«çš„æ‰€æœ‰é¢„æµ‹
        class_mask = [p['predicted_class'] == class_idx for p in sample_predictions]
        
        if any(class_mask):
            class_confidences = [p['confidence'] for p, mask in zip(sample_predictions, class_mask) if mask]
            class_stats[class_names[class_idx]] = {
                'count': sum(class_mask),
                'avg_confidence': np.mean(class_confidences) if class_confidences else 0,
                'std_confidence': np.std(class_confidences) if class_confidences else 0,
                'high_confidence_count': sum(c >= confidence_threshold for c in class_confidences)
            }
        else:
            class_stats[class_names[class_idx]] = {
                'count': 0,
                'avg_confidence': 0,
                'std_confidence': 0,
                'high_confidence_count': 0
            }
    
    # æ‰¾å‡ºæ¨¡å‹æœ€"è‡ªä¿¡"å’Œæœ€"ä¸è‡ªä¿¡"çš„ç±»åˆ«
    if class_stats:
        sorted_by_confidence = sorted(class_stats.items(), 
                                     key=lambda x: x[1]['avg_confidence'], 
                                     reverse=True)
        most_confident = sorted_by_confidence[0][0] if sorted_by_confidence else "None"
        least_confident = sorted_by_confidence[-1][0] if sorted_by_confidence else "None"
        
        sorted_by_count = sorted(class_stats.items(), 
                                key=lambda x: x[1]['count'], 
                                reverse=True)
        most_predicted = sorted_by_count[0][0] if sorted_by_count else "None"
        least_predicted = sorted_by_count[-1][0] if sorted_by_count else "None"
    else:
        most_confident = least_confident = most_predicted = least_predicted = "None"
    
    return {
        'class_stats': class_stats,
        'most_confident_class': most_confident,
        'least_confident_class': least_confident,
        'most_predicted_class': most_predicted,
        'least_predicted_class': least_predicted,
        'overall_avg_confidence': np.mean(confidence_scores) if confidence_scores else 0,
        'high_confidence_ratio': sum(c >= confidence_threshold for c in confidence_scores) / len(confidence_scores) if confidence_scores else 0
    }

def plot_confusion_matrix_and_analysis(teacher_model, test_loader, device, 
                                      experiment_name="MeanTeacher"):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶è¿›è¡Œè¯¦ç»†åˆ†æ
    """
    teacher_model.eval()
    
    all_preds = []
    all_labels = []
    class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 
                  'dog', 'horse', 'monkey', 'ship', 'truck']
    
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = teacher_model(images)
            _, preds = torch.max(outputs, 1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds, normalize='true')
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'Confusion Matrix - {experiment_name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.tight_layout()
    plt.savefig(f'confusion_matrix_{experiment_name}.png', dpi=300)
    plt.close()
    
    # åˆ†ææ··æ·†çŸ©é˜µ
    analysis_results = {}
    
    for i in range(10):
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾åº¦å’Œå¬å›ç‡
        tp = cm[i, i]
        fp = np.sum(cm[:, i]) - tp
        fn = np.sum(cm[i, :]) - tp
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        analysis_results[class_names[i]] = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'true_positive': tp,
            'false_positive': fp,
            'false_negative': fn
        }
    
    # æ‰¾å‡ºæœ€å®¹æ˜“æ··æ·†çš„ç±»åˆ«å¯¹
    confusion_pairs = []
    for i in range(10):
        for j in range(10):
            if i != j and cm[i, j] > 0.05:  # è¯¯åˆ†ç±»ç‡å¤§äº5%
                confusion_pairs.append({
                    'from': class_names[i],
                    'to': class_names[j],
                    'rate': cm[i, j]
                })
    
    confusion_pairs.sort(key=lambda x: x['rate'], reverse=True)
    
    # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®F1åˆ†æ•°
    if analysis_results:
        best_class = max(analysis_results.items(), key=lambda x: x[1]['f1_score'])[0]
        worst_class = min(analysis_results.items(), key=lambda x: x[1]['f1_score'])[0]
    else:
        best_class = worst_class = "None"
    
    return {
        'confusion_matrix': cm,
        'class_analysis': analysis_results,
        'top_confusion_pairs': confusion_pairs[:5] if confusion_pairs else [],
        'best_class': best_class,
        'worst_class': worst_class
    }

# ========== åŸæœ‰å‡½æ•° ==========

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device():
    """è®¾ç½®è®¾å¤‡å¹¶ä¼˜åŒ–GPUæ€§èƒ½"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        
        # GPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®
        torch.backends.cudnn.benchmark = True  # åŠ é€Ÿå·ç§¯è¿ç®—
        torch.backends.cudnn.deterministic = False  # ä¸ºäº†é€Ÿåº¦ç‰ºç‰²å¯é‡å¤æ€§
        
        gpu_props = torch.cuda.get_device_properties(0)
        print(f"âœ… ä½¿ç”¨GPU: {gpu_props.name}")
        print(f"   GPUå†…å­˜: {gpu_props.total_memory / 1024**3:.1f} GB")
        print(f"   CUDAæ ¸å¿ƒ: {gpu_props.multi_processor_count}")
        
    else:
        device = torch.device('cpu')
        print("âš ï¸ ä½¿ç”¨CPU (GPUä¸å¯ç”¨)")
    
    return device

def log_gradient_info(model, optimizer, epoch, batch_idx, log_freq=50):
    """è®°å½•æ¢¯åº¦ä¿¡æ¯"""
    if batch_idx % log_freq != 0:
        return
    
    total_norm = 0.0
    max_grad = -float('inf')
    min_grad = float('inf')
    num_params = 0
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            param_norm = param.grad.data.norm(2).item()
            total_norm += param_norm ** 2
            
            param_max = param.grad.data.max().item()
            param_min = param.grad.data.min().item()
            max_grad = max(max_grad, param_max)
            min_grad = min(min_grad, param_min)
            
            num_params += 1
    
    if num_params > 0:
        total_norm = total_norm ** 0.5
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"Epoch {epoch+1}, Batch {batch_idx}: "
              f"æ¢¯åº¦èŒƒæ•°={total_norm:.4f}, LR={current_lr:.6f}, "
              f"æœ€å¤§æ¢¯åº¦={max_grad:.6f}, æœ€å°æ¢¯åº¦={min_grad:.6f}")
        
        # æ¢¯åº¦å¼‚å¸¸è­¦å‘Š
        if total_norm > 100:
            print("âš ï¸  è­¦å‘Š: æ¢¯åº¦å¯èƒ½çˆ†ç‚¸ (èŒƒæ•° > 100)")
        elif total_norm < 0.0001:
            print("âš ï¸  è­¦å‘Š: æ¢¯åº¦å¯èƒ½æ¶ˆå¤± (èŒƒæ•° < 0.0001)")

def train_mean_teacher(optimizer_name='adam', learning_rate=0.001, batch_size=64, epochs=50,
                      use_amp=True, model_type='resnet18', validation_split=0.1,
                      consistency_weight=10.0, ema_decay=0.999, warmup_epochs=10,
                      use_kl_loss=False, max_grad_norm=1.0, save_best=True,
                      experiment_name="MeanTeacher", num_workers=None):
    """Mean TeacheråŠç›‘ç£è®­ç»ƒ"""
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    device = setup_device()
    
    # è·å–æ•°æ®ï¼ˆåŒ…å«æ— æ ‡ç­¾æ•°æ®ï¼‰ï¼Œæ˜ç¡®æŒ‡å®šnum_workers
    train_loader, val_loader, test_loader, unlabeled_loader = get_stl10_dataloaders(
        batch_size=batch_size,
        use_resnet_preprocessing=True,
        validation_split=validation_split,
        include_unlabeled=True,
        num_workers=num_workers  # ä¼ é€’å‚æ•°
    )
    
    # åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹
    print("åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹...")
    student_model = STL10_ResNet18(pretrained=True, feature_extract=False)
    teacher_model = STL10_ResNet18(pretrained=True, feature_extract=False)
    
    student_model = student_model.to(device)
    teacher_model = teacher_model.to(device)
    
    # åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹æƒé‡ä¸ºå­¦ç”Ÿæ¨¡å‹
    teacher_model.load_state_dict(student_model.state_dict())
    
    # ç¡®ä¿æ•™å¸ˆæ¨¡å‹ä¸è®¡ç®—æ¢¯åº¦
    for param in teacher_model.parameters():
        param.requires_grad = False
    teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹å§‹ç»ˆåœ¨evalæ¨¡å¼
    
    # ä¼˜åŒ–å™¨åªæ›´æ–°å­¦ç”Ÿæ¨¡å‹
    weight_decay = 1e-4
    
    if optimizer_name.lower() == 'adam':
        optimizer = optim.Adam(student_model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'adamw':
        # AdamWéœ€è¦ä¸åŒçš„æƒé‡è¡°å‡
        optimizer = optim.AdamW(
            student_model.parameters(), 
            lr=learning_rate, 
            weight_decay=0.01,  # å¢åŠ æƒé‡è¡°å‡
            betas=(0.9, 0.999),
            eps=1e-8
        )
    elif optimizer_name.lower() == 'sgd':
        optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'rmsprop':
        optimizer = optim.RMSprop(
            student_model.parameters(), 
            lr=learning_rate, 
            alpha=0.99,
            momentum=0.0,
            weight_decay=weight_decay,
            eps=1e-8
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
    
    # æŸå¤±å‡½æ•°
    supervised_criterion = nn.CrossEntropyLoss()
    
    # ä¸€è‡´æ€§æŸå¤±é€‰æ‹©
    if use_kl_loss:
        # ä½¿ç”¨å¯¹ç§°KLæ•£åº¦
        consistency_criterion = nn.KLDivLoss(reduction='batchmean')
        print("ä½¿ç”¨KLæ•£åº¦ä½œä¸ºä¸€è‡´æ€§æŸå¤±")
    else:
        # ä½¿ç”¨å‡æ–¹è¯¯å·®
        consistency_criterion = nn.MSELoss()
        print("ä½¿ç”¨MSEä½œä¸ºä¸€è‡´æ€§æŸå¤±")
    
    print(f"\nå¼€å§‹Mean Teacherè®­ç»ƒ...")
    print(f"å®éªŒåç§°: {experiment_name}")
    print(f"ä¼˜åŒ–å™¨: {optimizer_name}")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print(f"ä¸€è‡´æ€§æƒé‡: {consistency_weight}")
    print(f"EMAè¡°å‡ç‡: {ema_decay}")
    print(f"çƒ­èº«epochs: {warmup_epochs}")
    print(f"è®¾å¤‡: {device}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"æ¢¯åº¦è£å‰ª: {max_grad_norm}")
    
    # è®­ç»ƒè®°å½•
    train_losses = []
    val_accuracies = []
    supervised_losses = []
    consistency_losses = []
    training_times = []
    learning_rates = []
    gradient_norms = []  # è®°å½•æ¢¯åº¦èŒƒæ•°
    
    # ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹
    best_val_accuracy = 0.0
    best_model_state = None
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        epoch_start_time = time.time()
        student_model.train()
        
        running_supervised_loss = 0.0
        running_consistency_loss = 0.0
        running_total_loss = 0.0
        
        # è®¡ç®—å½“å‰epochçš„ä¸€è‡´æ€§æƒé‡ï¼ˆçƒ­èº«é˜¶æ®µçº¿æ€§å¢åŠ ï¼‰
        if epoch < warmup_epochs:
            current_consistency_weight = consistency_weight * ((epoch + 1) / warmup_epochs) ** 2
        else:
            current_consistency_weight = consistency_weight
        
        # å‡†å¤‡æ— æ ‡ç­¾æ•°æ®è¿­ä»£å™¨
        if unlabeled_loader:
            unlabeled_iter = iter(unlabeled_loader)
        else:
            unlabeled_iter = None
        
        # è®­ç»ƒæ‰¹æ¬¡
        for batch_idx, (labeled_images, labels) in enumerate(train_loader):
            labeled_images, labels = labeled_images.to(device), labels.to(device)
            
            # è·å–æ— æ ‡ç­¾æ•°æ®æ‰¹æ¬¡
            weak_images = None
            strong_images = None
            
            if unlabeled_iter is not None:
                try:
                    batch_data = next(unlabeled_iter)
                    
                    # æ ¹æ®æ‚¨çš„æ•°æ®ç»“æ„ï¼Œbatch_dataæ˜¯åˆ—è¡¨ï¼š[images_list, labels]
                    if isinstance(batch_data, list) and len(batch_data) == 2:
                        images_list = batch_data[0]
                        if isinstance(images_list, list) and len(images_list) == 2:
                            weak_images = images_list[0]  # å¼±å¢å¼ºå›¾åƒ
                            strong_images = images_list[1]  # å¼ºå¢å¼ºå›¾åƒ
                    
                    if weak_images is not None and strong_images is not None:
                        weak_images = weak_images.to(device)
                        strong_images = strong_images.to(device)
                        
                except StopIteration:
                    # é‡æ–°åˆå§‹åŒ–æ— æ ‡ç­¾æ•°æ®è¿­ä»£å™¨
                    unlabeled_iter = iter(unlabeled_loader)
                    try:
                        batch_data = next(unlabeled_iter)
                        if isinstance(batch_data, list) and len(batch_data) == 2:
                            images_list = batch_data[0]
                            if isinstance(images_list, list) and len(images_list) == 2:
                                weak_images = images_list[0]
                                strong_images = images_list[1]
                                weak_images = weak_images.to(device)
                                strong_images = strong_images.to(device)
                    except:
                        weak_images = strong_images = None
            
            # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.amp.autocast('cuda', enabled=use_amp):
                # ====== æœ‰æ ‡ç­¾æ•°æ®ç›‘ç£æŸå¤± ======
                student_labeled_outputs = student_model(labeled_images)
                supervised_loss = supervised_criterion(student_labeled_outputs, labels)
                
                # ====== æ— æ ‡ç­¾æ•°æ®ä¸€è‡´æ€§æŸå¤± ======
                consistency_loss = 0.0
                if weak_images is not None and strong_images is not None:
                    # å­¦ç”Ÿæ¨¡å‹é¢„æµ‹ï¼ˆä½¿ç”¨å¼ºå¢å¼ºï¼‰
                    student_unlabeled_outputs = student_model(strong_images)
                    
                    # æ•™å¸ˆæ¨¡å‹é¢„æµ‹ï¼ˆä½¿ç”¨å¼±å¢å¼ºï¼Œä¸è®¡ç®—æ¢¯åº¦ï¼‰
                    with torch.no_grad():
                        teacher_unlabeled_outputs = teacher_model(weak_images)
                    
                    # è®¡ç®—ä¸€è‡´æ€§æŸå¤±
                    if use_kl_loss:
                        # KLæ•£åº¦éœ€è¦log_softmaxè¾“å…¥
                        student_log_probs = torch.log_softmax(student_unlabeled_outputs, dim=1)
                        teacher_probs = torch.softmax(teacher_unlabeled_outputs, dim=1)
                        consistency_loss = consistency_criterion(student_log_probs, teacher_probs)
                    else:
                        # MSEä½¿ç”¨æ¦‚ç‡åˆ†å¸ƒ
                        student_probs = torch.softmax(student_unlabeled_outputs, dim=1)
                        teacher_probs = torch.softmax(teacher_unlabeled_outputs, dim=1)
                        consistency_loss = consistency_criterion(student_probs, teacher_probs)
                
                # ====== æ€»æŸå¤± ======
                total_loss = supervised_loss + current_consistency_weight * consistency_loss
            
            # ====== åå‘ä¼ æ’­å’Œä¼˜åŒ– ======
            optimizer.zero_grad()
            scaler.scale(total_loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            if max_grad_norm is not None:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_grad_norm)
            
            scaler.step(optimizer)
            scaler.update()
            
            # ====== æ¢¯åº¦ç›‘æ§ ======
            if batch_idx % 50 == 0:
                log_gradient_info(student_model, optimizer, epoch, batch_idx, log_freq=50)
            
            # ====== æ›´æ–°æ•™å¸ˆæ¨¡å‹ï¼ˆEMAï¼‰ ======
            with torch.no_grad():
                for teacher_param, student_param in zip(teacher_model.parameters(), student_model.parameters()):
                    teacher_param.data.mul_(ema_decay).add_(student_param.data, alpha=1 - ema_decay)
            
            # ====== è®°å½•æŸå¤± ======
            running_supervised_loss += supervised_loss.item()
            running_consistency_loss += consistency_loss.item() if (weak_images is not None and strong_images is not None) else 0
            running_total_loss += total_loss.item()
            
            # æ¯50ä¸ªbatchæ‰“å°è¿›åº¦
            if batch_idx % 50 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f'Epoch: {epoch+1}/{epochs} | Batch: {batch_idx}/{len(train_loader)} | '
                      f'Supervised Loss: {supervised_loss.item():.4f} | '
                      f'Consistency Loss: {consistency_loss.item():.4f} | '
                      f'Total Loss: {total_loss.item():.4f} | LR: {current_lr:.6f} | '
                      f'Consistency Weight: {current_consistency_weight:.2f}')
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        learning_rates.append(current_lr)
        
        # ====== éªŒè¯ ======
        teacher_model.eval()
        accuracy, _ = evaluate_model(teacher_model, val_loader, device)
        val_accuracies.append(accuracy)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if save_best and accuracy > best_val_accuracy:
            best_val_accuracy = accuracy
            best_model_state = {
                'teacher_state_dict': teacher_model.state_dict(),
                'student_state_dict': student_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'accuracy': accuracy,
                'epoch': epoch
            }
            torch.save(best_model_state, f'best_model_{experiment_name}.pth')
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}%")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_supervised_loss = running_supervised_loss / len(train_loader)
        avg_consistency_loss = running_consistency_loss / len(train_loader)
        avg_total_loss = running_total_loss / len(train_loader)
        
        supervised_losses.append(avg_supervised_loss)
        consistency_losses.append(avg_consistency_loss)
        train_losses.append(avg_total_loss)
        
        epoch_time = time.time() - epoch_start_time
        training_times.append(epoch_time)
        
        print(f'Epoch [{epoch+1}/{epochs}] | æ—¶é—´: {epoch_time:.1f}s | '
              f'ç›‘ç£æŸå¤±: {avg_supervised_loss:.4f} | ä¸€è‡´æ€§æŸå¤±: {avg_consistency_loss:.4f} | '
              f'æ€»æŸå¤±: {avg_total_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {accuracy:.2f}% | '
              f'ä¸€è‡´æ€§æƒé‡: {current_consistency_weight:.2f} | LR: {current_lr:.6f}')
    
    # ====== æœ€ç»ˆæµ‹è¯•å’Œåˆ†æ ======
    if best_model_state and os.path.exists(f'best_model_{experiment_name}.pth'):
        checkpoint = torch.load(f'best_model_{experiment_name}.pth')
        teacher_model.load_state_dict(checkpoint['teacher_state_dict'])
        print(f"åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯• (epoch {checkpoint['epoch']+1})")
    
    teacher_model.eval()
    final_accuracy, class_accuracy = evaluate_model(teacher_model, test_loader, device)
    
    # ====== æ–°å¢ï¼šè¯¦ç»†ç±»åˆ«åˆ†æ ======
    print(f"\nğŸ” å¼€å§‹è¯¦ç»†ç±»åˆ«åˆ†æ...")
    
    # 1. æµ‹è¯•é›†ç±»åˆ«å‡†ç¡®ç‡åˆ†æ
    class_accuracies, best_class, worst_class, _ = analyze_class_performance(
        teacher_model, test_loader, device
    )
    
    print(f"\nğŸ“Š æµ‹è¯•é›†ç±»åˆ«å‡†ç¡®ç‡æ’å:")
    for class_name, acc in sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True):
        print(f"  {class_name}: {acc:.2f}%")
    
    print(f"\nğŸ† æœ€ä½³åˆ†ç±»ç±»åˆ«: {best_class[0]} ({best_class[1]:.2f}%)")
    print(f"ğŸ“‰ æœ€å·®åˆ†ç±»ç±»åˆ«: {worst_class[0]} ({worst_class[1]:.2f}%)")
    
    # 2. æ— æ ‡ç­¾æ•°æ®é¢„æµ‹åˆ†æ
    if unlabeled_loader:
        unlabeled_analysis = analyze_unlabeled_data_performance(
            teacher_model, unlabeled_loader, device,
            confidence_threshold=0.8,
            num_samples=2000
        )
        
        print(f"\nğŸ“Š æ— æ ‡ç­¾æ•°æ®é¢„æµ‹åˆ†æ:")
        print(f"  æ•´ä½“å¹³å‡ç½®ä¿¡åº¦: {unlabeled_analysis['overall_avg_confidence']:.3f}")
        print(f"  é«˜ç½®ä¿¡åº¦é¢„æµ‹æ¯”ä¾‹: {unlabeled_analysis['high_confidence_ratio']:.3f}")
        print(f"  æ¨¡å‹æœ€è‡ªä¿¡çš„ç±»åˆ«: {unlabeled_analysis['most_confident_class']}")
        print(f"  æ¨¡å‹æœ€ä¸è‡ªä¿¡çš„ç±»åˆ«: {unlabeled_analysis['least_confident_class']}")
        
        print(f"\nğŸ“ˆ æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹ç»Ÿè®¡:")
        for class_name, stats in unlabeled_analysis['class_stats'].items():
            print(f"  {class_name}: é¢„æµ‹æ¬¡æ•°={stats['count']}, "
                  f"å¹³å‡ç½®ä¿¡åº¦={stats['avg_confidence']:.3f}, "
                  f"é«˜ç½®ä¿¡åº¦={stats['high_confidence_count']}")
    else:
        unlabeled_analysis = None
        print("\nâš ï¸  æ— æ— æ ‡ç­¾æ•°æ®å¯ç”¨äºåˆ†æ")
    
    # 3. æ··æ·†çŸ©é˜µåˆ†æ
    confusion_analysis = plot_confusion_matrix_and_analysis(
        teacher_model, test_loader, device, experiment_name
    )
    
    print(f"\nğŸ” æ··æ·†çŸ©é˜µåˆ†æ:")
    print(f"  æœ€ä½³F1åˆ†æ•°ç±»åˆ«: {confusion_analysis['best_class']}")
    print(f"  æœ€å·®F1åˆ†æ•°ç±»åˆ«: {confusion_analysis['worst_class']}")
    
    print(f"\nğŸ”— æœ€æ˜“æ··æ·†çš„ç±»åˆ«å¯¹:")
    for pair in confusion_analysis['top_confusion_pairs']:
        print(f"  {pair['from']} â†’ {pair['to']}: {pair['rate']:.3f}")
    
    avg_epoch_time = sum(training_times) / len(training_times)
    print(f"\nè®­ç»ƒå®Œæˆ!")
    print(f"å¹³å‡æ¯è½®è®­ç»ƒæ—¶é—´: {avg_epoch_time:.1f}ç§’")
    print(f"æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡: {final_accuracy:.2f}%")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
    
    return {
        'student_model': student_model,
        'teacher_model': teacher_model,
        'train_losses': train_losses,
        'supervised_losses': supervised_losses,
        'consistency_losses': consistency_losses,
        'val_accuracies': val_accuracies,
        'final_accuracy': final_accuracy,
        'class_accuracy': class_accuracy,
        'training_times': training_times,
        'learning_rates': learning_rates,
        'best_val_accuracy': best_val_accuracy,
        'experiment_name': experiment_name,
        'gradient_norms': gradient_norms,
        # æ–°å¢çš„åˆ†æç»“æœ
        'detailed_class_accuracies': class_accuracies,
        'best_class': best_class,
        'worst_class': worst_class,
        'unlabeled_analysis': unlabeled_analysis if unlabeled_loader else None,
        'confusion_analysis': confusion_analysis
    }

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡ŒMean TeacheråŠç›‘ç£å®éªŒ"""
    
    print("STL-10 å›¾åƒåˆ†ç±»å®éªŒ - Mean TeacheråŠç›‘ç£å­¦ä¹ ")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()}")
    print("Python 3.9 + PyTorch 2.7.1 + CUDA 12.8")
    print("="*60)
    
    # Mean Teacherå®éªŒé…ç½®
    mean_teacher_experiments = [
        {
            'name': 'MeanTeacher_Adam', 
            'optimizer': 'adam', 
            'lr': 0.0001, 
            'batch_size': 64, 
            'epochs': 50,
            'consistency_weight': 10.0,
            'ema_decay': 0.999,
            'use_kl_loss': False,
            'max_grad_norm': 1.0,
            'num_workers': 0
        },
        {
            'name': 'MeanTeacher_AdamW_Fixed', 
            'optimizer': 'adamw', 
            'lr': 0.0002,  # æé«˜å­¦ä¹ ç‡ï¼ˆåŸä¸º0.0001ï¼‰->0.001æ€»æŸå¤±å¾ˆé«˜ï¼Œä¸€è‡´æ€§æŸå¤±é«˜ï¼Œé™ä½å­¦ä¹ è‡³0.0005->lossæ³¢åŠ¨å¤ªå¤§å­¦ä¹ ç‡å†é™åˆ°0.0002
            'batch_size': 64, 
            'epochs': 50,
            'consistency_weight': 1.0,  # é™ä½ä¸€è‡´æ€§æƒé‡ï¼ˆåŸä¸º10.0ï¼‰-ã€‹å†é™ä½ä¸€è‡´æ€§æƒé‡
            'ema_decay': 0.997,
            # å°è¯•MSEæŸå¤±
            'use_kl_loss': True,
            'max_grad_norm': 1.0,  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ªï¼Œä»1.0é™ä½0.5->0.5å¤ªä¸¥æ ¼çš„è£å‰ªä¼šæ‰­æ›²æ¢¯åº¦æ–¹å‘
            'num_workers': 0
        },
        {
            'name': 'MeanTeacher_SGD', 
            'optimizer': 'sgd', 
            'lr': 0.01, 
            'batch_size': 64, 
            'epochs': 50,
            'consistency_weight': 10.0,
            'ema_decay': 0.999,
            'use_kl_loss': False,
            'max_grad_norm': 1.0,
            'num_workers': 0
        }
    ]
    
    results = {}
    
    for exp_config in mean_teacher_experiments:
        print(f"\n{'='*50}")
        print(f"å®éªŒ: {exp_config['name']}")
        print(f"{'='*50}")
        
        result = train_mean_teacher(
            optimizer_name=exp_config['optimizer'],
            learning_rate=exp_config['lr'],
            batch_size=exp_config['batch_size'],
            epochs=exp_config['epochs'],
            use_amp=True,
            model_type='resnet18',
            validation_split=0.1,
            consistency_weight=exp_config['consistency_weight'],
            ema_decay=exp_config['ema_decay'],
            warmup_epochs=10,
            use_kl_loss=exp_config['use_kl_loss'],
            max_grad_norm=exp_config['max_grad_norm'],
            experiment_name=exp_config['name'],
            num_workers=exp_config['num_workers']
        )
        
        results[exp_config['name']] = result
        
        print(f"\nğŸ“Š {exp_config['name']} æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {result['final_accuracy']:.2f}%")
        print(f"ğŸ“Š {exp_config['name']} æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {result['best_val_accuracy']:.2f}%")
    
    # ç»˜åˆ¶Mean Teacherè®­ç»ƒæ›²çº¿
    plot_mean_teacher_results(results)
    
    # æ¯”è¾ƒä¸åŒé…ç½®çš„æ€§èƒ½
    print_experiment_summary(results)
    
    # ä¿å­˜ç»“æœ
    with open('mean_teacher_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    print("Mean Teacherç»“æœå·²ä¿å­˜åˆ° mean_teacher_results.pkl")
    
    # æ€§èƒ½ç»Ÿè®¡
    total_training_time = sum(sum(result['training_times']) for result in results.values())
    print(f"\næ€»è®­ç»ƒæ—¶é—´: {total_training_time:.1f}ç§’ ({total_training_time/60:.1f}åˆ†é’Ÿ)")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_exp = max(results.items(), key=lambda x: x[1]['final_accuracy'])
    best_val_exp = max(results.items(), key=lambda x: x[1]['best_val_accuracy'])
    
    print(f"\nğŸ‰ æœ€ä½³Mean Teacheræ¨¡å‹ (æµ‹è¯•é›†): {best_exp[0]} - æµ‹è¯•å‡†ç¡®ç‡: {best_exp[1]['final_accuracy']:.2f}%")
    print(f"ğŸ‰ æœ€ä½³Mean Teacheræ¨¡å‹ (éªŒè¯é›†): {best_val_exp[0]} - éªŒè¯å‡†ç¡®ç‡: {best_val_exp[1]['best_val_accuracy']:.2f}%")
    
    # æ£€æŸ¥è¿‡æ‹Ÿåˆæƒ…å†µ
    print("\nè¿‡æ‹Ÿåˆæ£€æŸ¥:")
    check_overfitting(results)  # âœ… ç›´æ¥ä¼ å…¥æ•´ä¸ªresultså­—å…¸
    
    return results

if __name__ == "__main__":
    results = main()